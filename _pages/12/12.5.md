---
layout: post
title: "12.5 Maturity Roadmap – Evolving from Reactive Logs to Threat‑Informed Defence"
date: 2025-07-22
permalink: /modules/12/maturity-roadmap/
excerpt: >
  A structured, five‑stage journey that aligns ATT&CK coverage, governance
  controls, and blue‑team processes.  Use this roadmap to benchmark where you
  are today, justify budget for the next rung, and communicate progress to
  executives in language they trust.
header:
  overlay_image: /assets/images/maturity-banner.png
  overlay_color: "#2b2b2b"
  overlay_filter: "0.45"
---

> “Security maturity isn’t a switch you flip—it’s a staircase you climb, one
> ATT&CK tactic at a time.”  
> — Senior GRC Advisor

## 1 • Five‑Stage Maturity Model

| Level | Name                      | ATT&CK Coverage¹ | People / Process                           | Tooling Milestones                         |
|------:|---------------------------|-----------------:|--------------------------------------------|-------------------------------------------|
| **1** | **Reactive Logging**      | 0 – 15 %        | Ticket closes, no root‑cause               | Basic syslog/SIEM ingest                  |
| **2** | **Baseline Compliance**   | 15 – 40 %       | CIS/NIST checklist tracking                | Centralised log mgmt, weekly reports      |
| **3** | **Detection Engineering** | 40 – 70 %       | Sigma dev cycle, KPIs on precision         | EDR + Splunk/Elastic, rule CI pipeline    |
| **4** | **Threat‑Informed**       | 70 – 90 %       | CTI‑driven playbooks, purple‑team sprints  | ATT&CK Navigator layers, risk scoring     |
| **5** | **Autonomous Analytics**  | 90 – 100 %      | ML‑guided triage, continuous control tests | SOAR, ML scoring, auto gap‑ticketing      |

¹ *Coverage %* = techniques with validated detection ≥ 80 score (Module 10).

---

## 2 • Self‑Assessment Checklist

| Question                                   | Yes / No |
|--------------------------------------------|----------|
| Do we track **precision & recall** for every detection rule? |
| Can we generate a **Navigator heat‑map** in < 10 min?        |
| Is there a **CI job** that fails when coverage drops?        |
| Are **risk scores** tied to techniques and assets?           |
| Do we run **purple‑team** at least quarterly?                |

Count “Yes”.  
*0–1 Yes* → Level 1–2, *2–3* → Level 3, *4* → Level 4, *5* → Level 5.

---

## 3 • Roadmap Timeline (Example)

| Quarter | Key Deliverables                               | Target Level |
|---------|------------------------------------------------|--------------|
| **Q3 FY 25** | Deploy EDR, enable Sysmon 15 baseline        | → **Level 2** |
| **Q4 FY 25** | Build Sigma CI, first heat‑map               | → **Level 3** |
| **Q1 FY 26** | Purple‑team cadence, risk scoring dashboard | → **Level 4** |
| **Q3 FY 26** | SOAR auto‑ticket gaps, ML triage POC         | → **Level 5** |

Update after every sprint review; slide goes to the steering committee.

---

## 4 • Maturity JSON Template

```json
{
  "org": "NFSU‑SOC",
  "assessment_date": "2025‑07‑22",
  "level": 3,
  "coverage_pct": 58,
  "kpis": {
    "precision": 0.91,
    "mean_ttd_sec": 94,
    "gap_count": 27
  },
  "next_objectives": [
    "Implement weekly Atomic test CI",
    "Introduce risk‑score heat‑map to exec metrics"
  ]
}
```

Store in Git (`maturity.json`)—automation compares new commit vs last tag and
posts Slack alert when **`level`** increases.

---

## 5 • Communication Artefacts

| Stakeholder | Artefact                           | Cadence |
|-------------|------------------------------------|---------|
| Exec Board  | One‑page scorecard (Level, KPIs)   | Quarterly |
| SOC Leads   | Heat‑map diff + gap backlog        | Monthly |
| GRC / Audit | ATT&CK ↔ CSF/CIS mapping sheet     | Semi‑annual |
| DevOps      | High‑risk technique tickets        | Sprint |

Use consistent colour codes: Level 3 = **amber**, Level 4 = **teal**, Level 5 = **emerald**.

---

## 6 • Common Pitfalls & Remedies

| Pitfall                              | Fix                                              |
|--------------------------------------|--------------------------------------------------|
| **Tool > Process bias**              | Document SOPs; require PR for every rule change  |
| **Heat‑map vanity metrics**          | Tie >70 score to real alert fidelity KPIs        |
| **Maturity stagnation**              | Set SLA: “Level N+1 in 2 quarters or exec review”|
| **Over‑scoring coverage**            | Use *tested* detection only (Module 10.5)        |

---

## 7 • Best Practices

1. **R‑ACI**: Define roles (Responsible, Accountable, Consulted, Informed) per maturity deliverable.  
2. **Celebrate Levels**: Internal newsletter when moving up a level—boosts morale.  
3. **External Validation**: Run MITRE Engenuity eval scripts annually for independent proof.  
4. **Budget Alignment**: Map next‑level objectives to CapEx / OpEx lines early.  
5. **Iterate, Don’t Boil the Ocean**: Two controls improved per sprint outrun big‑bang projects.

---

<div class="post-resources container">
  <h3>Resources</h3>
  <ul>
    <li><a href="https://github.com/mitre/ctid-match-maturity" target="_blank">MITRE Threat‑Informed Maturity Model (reference)</a></li>
    <li><a href="https://www.first.org/epss" target="_blank">EPSS Data for Likelihood Weighting</a></li>
    <li><a href="https://mitre-attack.github.io/attack-navigator/" target="_blank">ATT&CK Navigator</a></li>
  </ul>
</div>

<a href="{{ site.baseurl }}/modules/12/automated-control-audits/" class="next-link">12.6 Automated Control Audits →</a>