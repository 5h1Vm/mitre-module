---
layout: post
title: "10.5 Rule Testing & Tuning"
date: 2025-07-10
permalink: /modules/10/rule-testing-tuning/
excerpt: >
  A detection rule isn’t “done” until it fires on emulated attacks, ignores
  benign noise, and is measured for precision and recall.  This post builds a
  repeatable pipeline—using Atomic Red Team, Caldera, and Splunk/Elastic
  validation scripts—that drives continuous improvement of your ATT&CK‑mapped
  analytics.
header:
  overlay_image: /assets/images/rule-testing-banner.png
  overlay_color: "#2b2b2b"
  overlay_filter: "0.4"
---

> “Write, test, fail, tune, repeat—detection engineering is software
> engineering with real adversaries.”  
> — Principal Blue‑Team Engineer

## 1 • Testing Philosophy

| Stage               | Purpose                              | ATT&CK Tie‑in                    |
|---------------------|--------------------------------------|----------------------------------|
| **Emulation**       | Replay atomic TTPs                   | Validates technique coverage     |
| **Assertion**       | Verify alert fires                   | Confirms mapping & log sources   |
| **Noise Check**     | Run baseline workload (office apps)  | Ensures low FP rate              |
| **Metric Capture**  | Precision / recall, latency          | Quantifies detection health      |

Goal: each **Sigma/SPL/EQL** rule has an automated unit‑test suite.

---

## 2 • Lab Tooling Matrix

| Tool / Framework      | Role in Pipeline         | Output Consumed by SIEM |
|-----------------------|--------------------------|-------------------------|
| **Atomic Red Team**   | Single‑technique tests   | Sysmon, Windows Event logs |
| **MITRE Caldera**     | End‑to‑end adversary OPS | Sysmon, Zeek, EDR JSON  |
| **PurpleSharp**       | C# in‑memory atomic exec | ETW, Security.evtx      |
| **DetectionLab**      | Pre‑built Vagrant range  | All the above           |

> **Tip:** Tag each emulation run with a unique `GUID` in command line so the
trace can be grepped in logs.

---

## 3 • Automated Test Harness (Splunk Example)

```bash
# 1. Kick Atomic test inside Windows guest
invoke-atomictest T1059.003 -GetPrereqs -Execute -Cleanup -Kill
# 2. Wait 30 s then query Splunk for rule hit
splunk search \
  "savedsearch CAR-CmdShell_Test | where marker_guid=\"${GUID}\"" \
  -auth admin:$PW -maxout 0 > result.json
# 3. Assert non‑zero hits
[[ $(jq '.results | length' result.json) -ge 1 ]] \
  || echo "Rule failed!" && exit 1
```

CI job fails if the rule does not trigger.

---

## 4 • Precision / Recall Metrics

| Metric          | Formula                          | Target |
| --------------- | -------------------------------- | ------ |
| **Precision**   | TP ÷ (TP + FP)                   | ≥ 0.9  |
| **Recall**      | TP ÷ (TP + FN)                   | ≥ 0.8  |
| **Latency (s)** | `alert_time – event_time` median | ≤ 30   |

Python snippet:

```python
precision = TP/(TP+FP)
recall    = TP/(TP+FN)
```

Store metrics in CI artifact and chart trendline in Grafana.

---

## 5 • Tuning Techniques

| Symptom                 | Fix Strategy                                         |
| ----------------------- | ---------------------------------------------------- |
| High FP on Dev builds   | Add `ImagePath NOT IN ("C:\\build\\")` filter        |
| Misses non‑English path | Wildcard casing / Unicode normalisation              |
| Long latency            | Set Splunk real‑time schedule, adjust `acceleration` |
| Partial recall          | Add sub‑techniques (`T1059.001`, `.003`, `.004`)     |

After tuning, rerun full emulation suite before merging to `main`.

---

## 6 • Timeline Walk‑Through

| Time UTC     | Action                                                         | Rule Status |
| ------------ | -------------------------------------------------------------- | ----------- |
| 08 : 01      | Caldera runs `T1021.002 SMB lateral`                           | —           |
| 08 : 02      | Sysmon 3 logs connection event                                 | —           |
| 08 : 02 : 07 | Splunk correlation search triggers `SMB Lateral Movement` rule | **Alert**   |
| 08 : 02 : 10 | SOAR isolates host in lab VLAN                                 | —           |

Latency = 3 s, precision measured at 1.0 (no FP).

---

## 7 • Continuous Tuning Loop

```mermaid
graph TD
A(Code Commit) --> B(CI Build + Unit Tests)
B -->|Pass| C(Stage to Dev SIEM)
C --> D(Range Regression Suite)
D -->|Pass 2 days| E(Prod Deploy)
D --|Fail| B
```

The rule must survive **two distinct test cycles** before production.

---

## 8 • Best Practices

* **Version every rule** — include `version:` header updated on each tuning PR.
* **Maintain Ground‑Truth Log Set** — 30 GB replay of benign traffic for FP tests.
* **Tag With Technique & CAR** — vital for coverage heat‑maps (§10.4).
* **Document Suppressions** — why exclusions exist and when to revisit.
* **Rotate Atomic Variants** — alter executables, paths, parameters weekly.

---

<div class="post-resources container">
  <h3>Resources</h3>
  <ul>
    <li><a href="https://github.com/redcanaryco/atomic-red-team" target="_blank">Atomic Red Team</a></li>
    <li><a href="https://github.com/mitre/caldera" target="_blank">MITRE Caldera</a></li>
    <li><a href="https://github.com/mvelazc0/PurpleSharp" target="_blank">PurpleSharp</a></li>
    <li><a href="https://detectionlab.network/" target="_blank">DetectionLab Range</a></li>
  </ul>
</div>

<a href="{{ site.baseurl }}/modules/10/sigma-to-yara/" class="next-link">10.6 Sigma → YARA‑L Pipeline →</a>