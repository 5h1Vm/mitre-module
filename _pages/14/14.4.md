---
layout: post
title: "14.4 AI / ML‑Driven Detection – Vector Search, RAG & Anomaly Models for ATT&CK Telemetry"
date: 2025-08-02
permalink: /modules/14/ai-ml-detection/
excerpt: >
  Machine‑learning augments traditional rules by clustering raw logs,
  embedding ATT&CK text, and surfacing *unknown* attacker behaviours.
  This lab builds an auto‑encoder for Sysmon, a RAG pipeline that
  turns GPT‑4o into a technique classifier, and a Chroma‑based vector
  store you can query in real time from Splunk and Elastic.
header:
  overlay_image: /assets/images/ai-banner.png
  overlay_color: "#2b2b2b"
  overlay_filter: "0.45"
---

> “Rules catch what we know; ML finds what we forgot to think about.”
> — Threat Analytics Data Scientist

---

## 1 • Use‑Cases Matrix

| ATT&CK Objective            | ML Pattern                         | Typical Model / Tool            |
|-----------------------------|------------------------------------|---------------------------------|
| **Unknown Lateral Movement**| Host‑based anomaly (auto‑encoder)  | PyTorch, Scikit‑learn           |
| **Natural‑Language Alerts** | RAG technique classification       | GPT‑4o + LangChain              |
| **High‑Fidelity Similarity**| Vector search on embedded logs     | SBERT → Chroma / Pinecone       |
| **Risk Prioritisation**     | Gradient‑boosted risk scorer       | XGBoost + feature store         |
| **Continuous Tuning**       | Online learning w/ label feedback  | River, Kafka Streams            |

---

## 2 • Anomaly Auto‑Encoder for Sysmon

### 2.1 Data Prep (Logstash → Parquet)

```bash
logstash -f sysmon_to_parquet.conf
```

Parquet schema: `eventid, sha256, parent_hash, cmdline_len, hour, user_sid`.

### 2.2 Model (PyTorch)

```python
import torch, torch.nn as nn, pandas as pd, numpy as np
X = pd.read_parquet("sysmon.parquet").to_numpy(dtype=np.float32)
model = nn.Sequential(
        nn.Linear(X.shape[1], 32), nn.ReLU(),
        nn.Linear(32, 8),          nn.ReLU(),
        nn.Linear(8, 32),          nn.ReLU(),
        nn.Linear(32, X.shape[1]))
loss_fn = nn.MSELoss(); optim = torch.optim.Adam(model.parameters(), 1e-3)
for epoch in range(15):
    optim.zero_grad()
    loss = loss_fn(model(torch.tensor(X)), torch.tensor(X))
    loss.backward(); optim.step()
print("Train MSE:", loss.item())
torch.save(model.state_dict(), "ae_sysmon.pt")
```

### 2.3 Scoring & Alert

```python
recon = model(torch.tensor(X_val)).detach().numpy()
mse   = np.mean((X_val - recon)**2, axis=1)
threshold = np.percentile(mse, 99.5)
alerts = np.where(mse > threshold)[0]
```

Send indices in `alerts` to Splunk HTTP Event Collector; map to `T1055`
(*Process Injection*) and `T1036` (*Masquerading*).

---

## 3 • RAG Technique Classifier

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 1.  Embed ATT&CK Tech Descriptions
attack_df = pd.read_json("attack_techniques.json")
db = Chroma.from_texts(
        attack_df.description.tolist(),
        embedding=OpenAIEmbeddings(),
        metadatas=[{"tech_id":t} for t in attack_df.tech_id])

# 2.  Build QA Chain
chain = RetrievalQA.from_chain_type(
          ChatOpenAI(model_name="gpt-4o-mini"),
          retriever=db.as_retriever())

alert = "powershell.exe downloaded and executed base64 blob via iex"
resp  = chain({"query": alert})
print(resp["result"])
# →  "T1059.001  (Command & Scripting Interpreter: PowerShell)"
```

Feed `tech_id` into SIEM field `attack.technique.id`; auto‑tag new alerts
without human mapping.

---

## 4 • Vector Search in Splunk

1. Install **splunk‑v (FAISS)** app.  
2. Index embeddings: `| tstats count FROM datamodel=Endpoint.Processes | vectorize`.  
3. Similarity search:

```spl
| vectorsearch field=embedding topk=5
  query="powershell encodedcommand new‑object net.webclient"
| table _time _raw similarity
```

Link results to Navigator layer via REST (`/api/layer/bulk`).

---

## 5 • Risk Model (Gradient Boost)

```python
import xgboost as xgb, json
df = pd.read_csv("detections.csv")   # label 1 breach, 0 benign
X,y = df.drop("label",axis=1), df.label
model = xgb.XGBClassifier(n_estimators=300, max_depth=4).fit(X,y)
expl = model.get_booster().get_score(importance_type='gain')
json.dump(expl, open("feature_importance.json","w"))
```

Top features → `risk_feature_map.csv` → Module 12.3 scorer.

---

## 6 • CI / CD Integration

| Stage       | Action                                       |
|-------------|----------------------------------------------|
| **Model CI**| Unit tests (shape, drift), MLOps tag `v1.4`  |
| **Pipeline**| Docker image push → Argo‑Rollout canary      |
| **Roll‑back**| Auto if precision < 90 % on shadow traffic   |
| **Labeling**| SOC analysts press “True / False” in Grafana panel – labels to Kafka topic |

---

## 7 • Best Practices

| Area                 | Recommendation                           |
|----------------------|------------------------------------------|
| **Explainability**   | Capture SHAP values per alert            |
| **PII Guardrails**   | Strip email/body before LLM prompt       |
| **Sampling**         | Down‑sample verbose events (Process Start)|
| **Retraining Cadence**| Monthly or when CV ≥ 0.12 drift          |
| **Cost**             | Cache embeddings locally; batch LLM calls|

---

<div class="post-resources container">
  <h3>Resources</h3>
  <ul>
    <li><a href="https://github.com/langchain-ai/langchain" target="_blank">LangChain</a></li>
    <li><a href="https://python.langchain.com/docs/integrations/vectorstores/chroma" target="_blank">Chroma Vector Store</a></li>
    <li><a href="https://xgboost.readthedocs.io/" target="_blank">XGBoost</a></li>
    <li><a href="https://mitre-ml.github.io" target="_blank">MITRE ML & ATT&CK Projects</a></li>
  </ul>
</div>

<a href="{{ site.baseurl }}/modules/14/community-contribution/" class="next-link">14.5 Community Contribution →</a>