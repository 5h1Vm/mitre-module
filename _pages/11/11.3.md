---
layout: post
title: "11.3 MITRE ATT&CK Evaluations – Mining Public Data to Tune Detections"
date: 2025-07-14
permalink: /modules/11/mitre-attack-evaluations/
excerpt: >
  MITRE Engenuity publishes step‑by‑step adversary emulation results for dozens
  of EDR vendors.  Learn how to extract the telemetry, map it to your own
  detections, and close visibility gaps without buying another tool.
header:
  overlay_image: /assets/images/mitre-evals-banner.png
  overlay_color: "#2b2b2b"
  overlay_filter: "0.4"
---

> “The evals are a gold mine—free ground‑truth traces for every ATT&CK step,
> ready to replay in the lab.”  
> — DFIR Detection Strategist

## 1 • What Are MITRE Evaluations?

| Round (Year) | Emulated Threat Actor | Tactic Coverage | Public Artefacts                                   |
|--------------|----------------------|-----------------|----------------------------------------------------|
| **APT3** (2018) | Chinese cyber‑espionage | 10 tactics | CSV timeline, JSON steps                           |
| **APT29** (2019) | Russian SVR          | 12 tactics | PCAPs, host logs, screenshots                      |
| **FIN7 / Carbanak** (2020) | Financial crime | 13 tactics | Event logs, detection notes                        |
| **Wizard Spider / Sandworm** (2021) | Ryuk & Triton | 14 tactics | Raw telemetry + vendor alert JSON                  |
| **Turla** (2022) | Snake implants       | 14 tactics | Command output, Sigma baseline                     |

All artefacts are **creative‑commons**, hosted at
`https://github.com/mitre-engenuity/attack-evals`.

---

## 2 • Download & Parse Evaluation Data

bash
# Clone APT29 data
git clone https://github.com/mitre-engenuity/attack-evals
cd attack-evals/apt29/analytics

# Convert Jupyter notebook output to CSV timeline
jupyter nbconvert --to=csv apt29_detection_results.ipynb \
                  --output timeline.csv


### CSV Columns

| `step` | `technique_id` | `detection_type` | `timestamp` | `details` |
|--------|----------------|------------------|-------------|-----------|

---

## 3 • Import into Splunk for Gap Analysis

splunk
| inputcsv timeline.csv
| eval eval_source="APT29"
| lookup local=true sigma_rule_map technique_id OUTPUT rule_name status
| eval gap=if(isnull(rule_name),"NO","YES")
| stats count BY tactic, gap


- **`sigma_rule_map`** is your rule catalogue from Module 10.  
- **Gap = NO** → Technique observed in evals **not** covered by your rules.

---

## 4 • Generate “Eval vs Own Coverage” Navigator Layer

python
import pandas as pd, json
from mitreattack.navlayers import Layer, Techniques

df  = pd.read_csv("timeline.csv")
own = pd.read_json("coverage_q3fy25.json")['techniques']

layer = Layer(name="APT29 Eval vs Coverage")
for t in df['technique_id'].unique():
    has_rule = any(o['techniqueID']==t for o in own)
    score    = 80 if has_rule else 20
    layer.techniques.append(Techniques(techniqueID=t, score=score))
layer.to_json("apt29_gap.json")


Colour scheme: **Green** (≥80) already covered; **Red** (<25) missing.

---

## 5 • Replay Evaluations in DetectionLab

bash
vagrant up           # spin up Windows + Splunk range
cd attack-evals/apt29/step1
.\execute_step.ps1   # runs T1566.001 phishing loader


1. Verify Sysmon/EDR logs ingest to Splunk.  
2. Check your Sigma rule hits.  
3. Iterate tuning until heat‑map turns green for that step.

---

## 6 • Metrics Dashboard

| KPI                              | Calc Method                              | Target |
|----------------------------------|------------------------------------------|--------|
| **Eval Technique Coverage %**    | `covered / total steps`                  | ≥ 85 % |
| **Mean Detection Latency (s)**   | `_time - eval_timestamp`                 | ≤ 30 s |
| **False Negatives**              | Eval step with `gap=YES` & `has_log=TRUE`| 0      |
| **False Positives** (lab)        | Alerts outside eval timestamp window     | < 3    |

Grafana datasource: Splunk JSON API → `eval_metrics` index.

---

## 7 • Using Eval Data for Purple‑Team

1. **Red‑Team** reproduces eval steps with same binaries.  
2. **Blue‑Team** runs live rules & gathers missed detections.  
3. Update Navigator layer collaboratively in real time.  
4. Produce after‑action report with improved coverage %.

---

## 8 • Best Practices & Gotchas

| Item                     | Recommendation                                          |
|--------------------------|---------------------------------------------------------|
| **Vendor Noise**         | Ignore proprietary alert labels—focus on raw telemetry |
| **Log Source Parity**    | Match eval sensors (Sysmon, Zeek) to your stack first  |
| **Time‑Zone Drift**      | Normalise eval timestamps to UTC before diff            |
| **Custom Payloads**      | Replace live malware with safe “echo” wrappers          |
| **Continuous Refresh**   | Re‑run diff when MITRE publishes new round or your rules change |

---

<div class="post-resources container">
  <h3>Resources</h3>
  <ul>
    <li><a href="https://github.com/mitre-engenuity/attack-evals" target="_blank">MITRE Engenuity Evaluations Repo</a></li>
    <li><a href="https://attackevals.mitre-engenuity.org/" target="_blank">Official Evaluations Portal</a></li>
    <li><a href="https://detectionlab.network/" target="_blank">DetectionLab Range</a></li>
    <li><a href="https://mitre-attack.github.io/attack-navigator/" target="_blank">ATT&CK Navigator</a></li>
  </ul>
</div>

<a href="{{ site.baseurl }}/modules/11/purple-team-workflow/" class="next-link">11.4 Purple‑Team Workflow →</a>